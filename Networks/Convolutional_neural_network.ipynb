{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "epochs = 4\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "PATH = '../data/saved_model/CNN_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root = '../data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root = '../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=num_workers)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
    "          'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5                        # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=120, bias=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # x : 28 * 28 * 1 (FashionMnist)\n",
    "            # https://github.com/zalandoresearch/fashion-mnist\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # x : 24 * 24 * 6\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # x : 14 * 14 * 6\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # x : 10 * 10 * 16\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4 * 4 * 16, 120),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.flatten\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 991] loss: 0.862112283706665\n",
      "[1, 1991] loss: 0.6580991744995117\n",
      "[1, 2991] loss: 0.14126253128051758\n",
      "[1, 3991] loss: 0.4368342161178589\n",
      "[1, 4991] loss: 0.45767736434936523\n",
      "[1, 5991] loss: 0.61182701587677\n",
      "[1, 6991] loss: 2.1416380405426025\n",
      "[1, 7991] loss: 0.7698377370834351\n",
      "[1, 8991] loss: 1.09584641456604\n",
      "[1, 9991] loss: 1.0421218872070312\n",
      "[1, 10991] loss: 0.2796977758407593\n",
      "[1, 11991] loss: 0.6221855878829956\n",
      "[1, 12991] loss: 0.29405903816223145\n",
      "[1, 13991] loss: 0.5076323747634888\n",
      "[1, 14991] loss: 0.47638750076293945\n",
      "[2, 991] loss: 0.08655273914337158\n",
      "[2, 1991] loss: 1.1924101114273071\n",
      "[2, 2991] loss: 0.09851384162902832\n",
      "[2, 3991] loss: 1.717237949371338\n",
      "[2, 4991] loss: 0.1193915605545044\n",
      "[2, 5991] loss: 2.1439764499664307\n",
      "[2, 6991] loss: 0.9709827899932861\n",
      "[2, 7991] loss: 0.754624605178833\n",
      "[2, 8991] loss: 0.3867439031600952\n",
      "[2, 9991] loss: 0.4707169532775879\n",
      "[2, 10991] loss: 0.5942692756652832\n",
      "[2, 11991] loss: 1.26173734664917\n",
      "[2, 12991] loss: 0.27968430519104004\n",
      "[2, 13991] loss: 0.03960120677947998\n",
      "[2, 14991] loss: 0.29080134630203247\n",
      "[3, 991] loss: 0.7167854905128479\n",
      "[3, 1991] loss: 0.25412678718566895\n",
      "[3, 2991] loss: 0.0748586654663086\n",
      "[3, 3991] loss: 0.27290546894073486\n",
      "[3, 4991] loss: 0.21657776832580566\n",
      "[3, 5991] loss: 1.3169668912887573\n",
      "[3, 6991] loss: 0.7895987629890442\n",
      "[3, 7991] loss: 0.872704029083252\n",
      "[3, 8991] loss: 0.29309773445129395\n",
      "[3, 9991] loss: 0.40886563062667847\n",
      "[3, 10991] loss: 0.9827291369438171\n",
      "[3, 11991] loss: 2.1021053791046143\n",
      "[3, 12991] loss: 0.23371338844299316\n",
      "[3, 13991] loss: 0.10754334926605225\n",
      "[3, 14991] loss: 3.1761856079101562\n",
      "[4, 991] loss: 1.2641081809997559\n",
      "[4, 1991] loss: 0.29551148414611816\n",
      "[4, 2991] loss: 0.6677727103233337\n",
      "[4, 3991] loss: 0.08675634860992432\n",
      "[4, 4991] loss: 0.7246733903884888\n",
      "[4, 5991] loss: 0.01753973960876465\n",
      "[4, 6991] loss: 0.7298485636711121\n",
      "[4, 7991] loss: 0.670005738735199\n",
      "[4, 8991] loss: 1.0024352073669434\n",
      "[4, 9991] loss: 1.941128134727478\n",
      "[4, 10991] loss: 1.493604063987732\n",
      "[4, 11991] loss: 1.3465741872787476\n",
      "[4, 12991] loss: 0.5438323020935059\n",
      "[4, 13991] loss: 0.1533370018005371\n",
      "[4, 14991] loss: 0.5986793041229248\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader, 0):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = net(images)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 990:\n",
    "            print(\"[{epoch}, {index}] loss: {loss}\".format(epoch = epoch + 1, index = i + 1, loss = loss.item()))\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 83.82\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(testloader, 0):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {}'.format(\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
